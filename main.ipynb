{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_chroma import Chroma\n",
    "import ollama\n",
    "import shutil\n",
    "import psutil\n",
    "import stat\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_path = \"chroma\"\n",
    "directory_path = \"C:/Users/aryan/Documents/LLMs/LLM-PDF-Reader/Documents\" # Replace this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory_path):\n",
    "    document_loader = PyPDFDirectoryLoader(directory_path)\n",
    "    return document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function = len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        filename = os.path.basename(source) if source else \"unknown\"\n",
    "        current_page_id = f\"{filename}:{page}\"\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "        last_page_id = current_page_id\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        chunk.metadata[\"id\"] = chunk_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_db(chunks: list[Document], persist_directory: str):\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=get_embeddings())\n",
    "\n",
    "    # Assign unique IDs to chunks\n",
    "    calculate_chunk_ids(chunks)  \n",
    "\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of Existing Chunks in DB: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"Number of New Chunks Added: {len(new_chunks)}\")\n",
    "\n",
    "        # Extract IDs from metadata\n",
    "        chunk_ids = [chunk.metadata[\"id\"] for chunk in chunks]\n",
    "\n",
    "        # Add documents with IDs\n",
    "        db.add_documents(chunks, ids=chunk_ids)\n",
    "        \n",
    "        # # Persist the database\n",
    "        # db.persist()\n",
    "    else:\n",
    "        print(\"No new chunks to add to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_database(persist_directory: str, reset=False):\n",
    "    \n",
    "    if not reset:\n",
    "        return\n",
    "    # Load the vector database (ChromaDB) from langchain_chroma\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=get_embeddings())\n",
    "\n",
    "    # Clear the collection\n",
    "    db.delete_collection()  # Clears the entire collection\n",
    "    print(\"Database cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query: str, persist_directory: str):\n",
    "    # Load the vector database (ChromaDB)\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=get_embeddings())\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query, k=5)\n",
    "\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an AI assistant. Use the provided context to answer the question accurately and concisely.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    If the context does not contain relevant information, respond with \"I don't know.\"\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = Ollama(model=\"mistral\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents(directory_path)\n",
    "chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_database(chroma_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='unintended exposure could lead to severe conse-\n",
      "quences, including breaches of private and sen-\n",
      "sitive information. This paper presents a black-\n",
      "box attack to force a RAG system to leak its\n",
      "private knowledge base which, differently from\n",
      "existing approaches, is adaptive and automatic.\n",
      "A relevance-based mechanism and an attacker-\n",
      "side open-source LLM favor the generation of\n",
      "effective queries to leak most of the (hidden)\n",
      "knowledge base. Extensive experimentation\n",
      "proves the quality of the proposed algorithm\n",
      "in different RAG pipelines and domains, com-\n",
      "paring to very recent related approaches, which\n",
      "turn out to be either not fully black-box, not\n",
      "adaptive, or not based on open-source models.\n",
      "The findings from our study remark the urgent\n",
      "need for more robust privacy safeguards in the' metadata={'source': 'C:\\\\Users\\\\aryan\\\\Documents\\\\LLMs\\\\LLM-PDF-Reader\\\\Documents\\\\Pirates_of_the_RAG.pdf', 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pirates_of_the_RAG.pdf:0:5\n"
     ]
    }
   ],
   "source": [
    "calculate_chunk_ids(chunks)\n",
    "id_example = chunks[5].metadata[\"id\"]\n",
    "print(id_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038177136331796646, 0.0329110249876976, -0.005459396634250879, 0.01436989288777113, -0.04029098153114319]\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings()\n",
    "vector = embeddings.embed_query(\"Hello, world!\")  # Generate an embedding\n",
    "print(vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Existing Chunks in DB: 136\n",
      "No new chunks to add to the database.\n"
     ]
    }
   ],
   "source": [
    "add_to_db(chunks, chroma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  RAG (Retrieval-Augmented Generation) is a system that enhances a language model's ability to generate responses by providing it with additional knowledge from a predefined database, often referred to as the knowledge base. This database can contain sensitive information and must be kept confidential to ensure privacy and security. RAG systems can be used in various applications such as customer support assistants, workflow streamlining within organizations, or medical support chatbots. The proposed algorithm allows a user with open-source tools to craft attacks on RAG systems, highlighting the need for more robust safeguards in their design. It's important to note that a RAG system is essentially an LLM (Language Learning Model) that generates text based on input prompts and retrieved information.\n",
      "Sources: ['Pirates_of_the_RAG.pdf:1:1', 'Pirates_of_the_RAG.pdf:0:3', 'Pirates_of_the_RAG.pdf:9:1', 'Pirates_of_the_RAG.pdf:3:5', 'Pirates_of_the_RAG.pdf:1:6']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" RAG (Retrieval-Augmented Generation) is a system that enhances a language model's ability to generate responses by providing it with additional knowledge from a predefined database, often referred to as the knowledge base. This database can contain sensitive information and must be kept confidential to ensure privacy and security. RAG systems can be used in various applications such as customer support assistants, workflow streamlining within organizations, or medical support chatbots. The proposed algorithm allows a user with open-source tools to craft attacks on RAG systems, highlighting the need for more robust safeguards in their design. It's important to note that a RAG system is essentially an LLM (Language Learning Model) that generates text based on input prompts and retrieved information.\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag(\"Explain RAG\", chroma_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
