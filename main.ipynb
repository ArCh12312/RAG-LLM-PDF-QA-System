{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_chroma import Chroma\n",
    "import ollama\n",
    "import shutil\n",
    "import psutil\n",
    "import stat\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_path = \"chroma\"\n",
    "directory_path = \"C:/Users/aryan/Documents/LLMs/LLM-PDF-Reader/Documents\" # Replace this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory_path):\n",
    "    document_loader = PyPDFDirectoryLoader(directory_path)\n",
    "    return document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function = len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        filename = os.path.basename(source) if source else \"unknown\"\n",
    "        current_page_id = f\"{filename}:{page}\"\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "        last_page_id = current_page_id\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        chunk.metadata[\"id\"] = chunk_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_db(chunks: list[Document], persist_directory: str):\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=get_embeddings())\n",
    "\n",
    "    # Assign unique IDs to chunks\n",
    "    calculate_chunk_ids(chunks)  \n",
    "\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of Existing Chunks in DB: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"Number of New Chunks Added: {len(new_chunks)}\")\n",
    "\n",
    "        # Extract IDs from metadata\n",
    "        chunk_ids = [chunk.metadata[\"id\"] for chunk in chunks]\n",
    "\n",
    "        # Add documents with IDs\n",
    "        db.add_documents(chunks, ids=chunk_ids)\n",
    "        \n",
    "        # # Persist the database\n",
    "        # db.persist()\n",
    "    else:\n",
    "        print(\"No new chunks to add to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_database(persist_directory: str, reset=False):\n",
    "    \n",
    "    if not reset:\n",
    "        return\n",
    "    # Load the vector database (ChromaDB) from langchain_chroma\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=get_embeddings())\n",
    "\n",
    "    # Clear the collection\n",
    "    db.delete_collection()  # Clears the entire collection\n",
    "    print(\"Database cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query: str, persist_directory: str):\n",
    "    # Load the vector database (ChromaDB)\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=get_embeddings())\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query, k=5)\n",
    "\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an AI assistant. Use the provided context to answer the question accurately and concisely.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    If the context does not contain relevant information, respond with \"I don't know.\"\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = Ollama(model=\"mistral\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents(directory_path)\n",
    "chunks = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_database(chroma_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='unintended exposure could lead to severe conse-\n",
      "quences, including breaches of private and sen-\n",
      "sitive information. This paper presents a black-\n",
      "box attack to force a RAG system to leak its\n",
      "private knowledge base which, differently from\n",
      "existing approaches, is adaptive and automatic.\n",
      "A relevance-based mechanism and an attacker-\n",
      "side open-source LLM favor the generation of\n",
      "effective queries to leak most of the (hidden)\n",
      "knowledge base. Extensive experimentation\n",
      "proves the quality of the proposed algorithm\n",
      "in different RAG pipelines and domains, com-\n",
      "paring to very recent related approaches, which\n",
      "turn out to be either not fully black-box, not\n",
      "adaptive, or not based on open-source models.\n",
      "The findings from our study remark the urgent\n",
      "need for more robust privacy safeguards in the' metadata={'source': 'C:\\\\Users\\\\aryan\\\\Documents\\\\LLMs\\\\LLM-PDF-Reader\\\\Documents\\\\Pirates_of_the_RAG.pdf', 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pirates_of_the_RAG.pdf:0:5\n"
     ]
    }
   ],
   "source": [
    "calculate_chunk_ids(chunks)\n",
    "id_example = chunks[5].metadata[\"id\"]\n",
    "print(id_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038177136331796646, 0.0329110249876976, -0.005459396634250879, 0.01436989288777113, -0.04029098153114319]\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings()\n",
    "vector = embeddings.embed_query(\"Hello, world!\")  # Generate an embedding\n",
    "print(vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Existing Chunks in DB: 0\n",
      "Number of New Chunks Added: 136\n"
     ]
    }
   ],
   "source": [
    "add_to_db(chunks, chroma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryan\\AppData\\Local\\Temp\\ipykernel_14524\\2972404512.py:27: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=\"mistral\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  RAG stands for Retrieval-Augmented Generation. It is a system that uses external knowledge or data to enhance an LLM (Language Model) by augmenting the input prompt with retrieved information. This is particularly useful when the system needs to respond to complex or specific questions, as it allows the model to provide more accurate and detailed answers. The knowledge base used in RAG systems often contains sensitive information that must be kept confidential to ensure privacy and security. Examples of where RAG systems can be deployed include customer support assistants, internal organizational tools, and medical support chatbots. The proposed algorithm in the given context allows a user to craft attacks on RAG systems and highlights the need for more robust safeguards in their design.\n",
      "Sources: ['Pirates_of_the_RAG.pdf:1:1', 'Pirates_of_the_RAG.pdf:0:3', 'Pirates_of_the_RAG.pdf:9:1', 'Pirates_of_the_RAG.pdf:3:5', 'Pirates_of_the_RAG.pdf:1:6']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' RAG stands for Retrieval-Augmented Generation. It is a system that uses external knowledge or data to enhance an LLM (Language Model) by augmenting the input prompt with retrieved information. This is particularly useful when the system needs to respond to complex or specific questions, as it allows the model to provide more accurate and detailed answers. The knowledge base used in RAG systems often contains sensitive information that must be kept confidential to ensure privacy and security. Examples of where RAG systems can be deployed include customer support assistants, internal organizational tools, and medical support chatbots. The proposed algorithm in the given context allows a user to craft attacks on RAG systems and highlights the need for more robust safeguards in their design.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag(\"Explain RAG\", chroma_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
